{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:39:10.422344Z",
     "iopub.status.busy": "2025-02-23T13:39:10.422122Z",
     "iopub.status.idle": "2025-02-23T13:39:17.753039Z",
     "shell.execute_reply": "2025-02-23T13:39:17.752106Z",
     "shell.execute_reply.started": "2025-02-23T13:39:10.422323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_size = 512  # Size of the input images\n",
    "# Training transformations\n",
    "common_train_transform = A.Compose([\n",
    "\t#A.PadIfNeeded(min_height=284, min_width=284, border_mode=0),  \n",
    "\tA.Resize(img_size, img_size),  \n",
    "\tA.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit= 20, p =0.5),\n",
    "\tA.CenterCrop(img_size, img_size, p=0.5),\n",
    "\t#A.Normalize(mean=[0.5], std=[0.5]),  # Normalize to [0,1] range\n",
    "\t#ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Validation transformations\n",
    "common_val_transform = A.Compose([\n",
    "\t#A.PadIfNeeded(min_height=284, min_width=284, border_mode=0),\n",
    "\tA.Resize(img_size, img_size),\n",
    "\t#A.Normalize(mean=[0.5], std=[0.5]),\n",
    "\t#ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Normalization for images\n",
    "transform_img = A.Compose([\n",
    "\t#A.Normalize(mean=[58.42], std=[51.01]),\n",
    "\tA.Normalize(mean=[0.5], std=[0.5]),\n",
    "\tToTensorV2(),\n",
    "])\n",
    "\n",
    "# Transform mask: Convert to binary mask and tensor\n",
    "transform_mask = A.Compose([\n",
    "\t#A.Lambda(image=lambda x: (x > 0).float()),  \n",
    "\tToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    " # This dataset loader will be used for experiment with single image based (MR1 or MR2) model , where model is encoder(narrow) + fc regression model   \n",
    "class ImageLoader_Dataset(Dataset):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\troot_dir=\"../../data/washu_all_img\",\n",
    "\t\tresponse_dir=\"../../data/PAT_imaging_record.xlsx\",\n",
    "\t\tphase=\"train\",\n",
    "\t\timg_transform=None,\n",
    "\t\tk_fold=5,\n",
    "\t\tfold=0,\n",
    "\t):\n",
    "\t\tself.root_dir = root_dir\n",
    "\t\tself.phase = phase\n",
    "\t\tself.img_transform = img_transform\n",
    "\n",
    "\t\t# Step 1: Read Excel sheet\n",
    "\t\tdf = pd.read_excel(response_dir, sheet_name=\"ROI STATS V3\")\n",
    "\t\tdf = df.dropna(subset=[\"Patient ID\", \"Side\", \"GT\"])\n",
    "\t\tdf[\"Patient ID\"] = df[\"Patient ID\"].astype(int).astype(str).str.strip()\n",
    "\t\tdf[\"Side\"] = df[\"Side\"].astype(str).str.strip()\n",
    "\t\tdf[\"GT\"] = pd.to_numeric(df[\"GT\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\t\tdf = df.dropna(subset=[\"GT\"])\n",
    "\t\tdf[\"GT\"] = df[\"GT\"].astype(int)\n",
    "\n",
    "\t\tlabel_map = {\n",
    "\t\t\t(\"p\" + row[\"Patient ID\"], row[\"Side\"]): row[\"GT\"]\n",
    "\t\t\tfor _, row in df.iterrows()\n",
    "\t\t}\n",
    "\n",
    "\t\t# Step 2: Get all patient IDs and apply k-fold\n",
    "\t\tall_patient_ids = sorted([\n",
    "\t\t\td for d in os.listdir(root_dir)\n",
    "\t\t\tif os.path.isdir(os.path.join(root_dir, d))\n",
    "\t\t])\n",
    "\n",
    "\t\t# kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "\t\t# splits = list(kf.split(all_patient_ids))\n",
    "\t\t# train_idx, val_idx = splits[fold]\n",
    "\n",
    "\t\tif phase == 'train':\n",
    "\t\t\tselected_patient_ids = all_patient_ids # [all_patient_ids[i] for i in train_idx]\n",
    "\t\telse:\n",
    "\t\t\tselected_patient_ids = all_patient_ids # [all_patient_ids[i] for i in val_idx]\n",
    "\n",
    "\n",
    "\t\t# Step 3: Gather all samples\n",
    "\t\tall_samples = []\n",
    "\t\tfor patient_id in selected_patient_ids:\n",
    "\t\t\tpatient_path = os.path.join(root_dir, patient_id)\n",
    "\t\t\tfor side in os.listdir(patient_path):\n",
    "\t\t\t\tside_path = os.path.join(patient_path, side)\n",
    "\t\t\t\tif not os.path.isdir(side_path):\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tkey = (patient_id, side)\n",
    "\t\t\t\tif key not in label_map:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tlabel = label_map[key]\n",
    "\t\t\t\tlabel = 1 if label == 0 else 0  # Optional: relabel if needed\n",
    "\n",
    "\t\t\t\tfor fname in os.listdir(side_path):\n",
    "\t\t\t\t\tif fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif')):\n",
    "\t\t\t\t\t\tfull_path = os.path.join(side_path, fname)\n",
    "\t\t\t\t\t\tif os.path.exists(full_path):\n",
    "\t\t\t\t\t\t\tall_samples.append((full_path, label))\n",
    "\n",
    "\t\tself.data = all_samples\n",
    "\t\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\timage_path, response = self.data[index]\n",
    "\n",
    "\t\t# Load image and mask as grayscale NumPy arrays\n",
    "\t\timage = np.array(Image.open(image_path).convert('L'))\n",
    "\n",
    "\t\t# Apply the corresponding transformations\n",
    "\t\tif self.phase == 'train':\n",
    "\t\t\ttransformed = common_train_transform(image=image)\n",
    "\t\telse:\n",
    "\t\t\ttransformed = common_val_transform(image=image)\n",
    "\n",
    "\t\tslice_image = transformed['image']\n",
    "\t\t# Apply additional transformations if provided\n",
    "\t\tif self.img_transform:\n",
    "\t\t\tslice_image = self.img_transform(image=slice_image)['image'].to(torch.float32)\n",
    "\t\t\tslice_image =  slice_image #transforms.functional.normalize(slice_image, mean=34.80, std= 42.51)\n",
    "\n",
    "\t\treturn slice_image, torch.tensor(response, dtype=torch.float32), image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:40:09.269397Z",
     "iopub.status.busy": "2025-02-23T13:40:09.269063Z",
     "iopub.status.idle": "2025-02-23T13:40:09.290500Z",
     "shell.execute_reply": "2025-02-23T13:40:09.289529Z",
     "shell.execute_reply.started": "2025-02-23T13:40:09.269371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "testDataset =  ImageLoader_Dataset(phase = 'test', img_transform= transform_img)\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "drop_last = False \n",
    "test_dataloader = DataLoader(\n",
    "            testDataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            drop_last= drop_last,\n",
    "            #persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_masks, image_path = next(iter(test_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Masks batch shape: {train_masks.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "# mask = train_masks[0].squeeze()\n",
    "# img_mask = np.ma.masked_where(mask == 0, mask)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (16,12))\n",
    "plt.imshow(img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:42:03.549386Z",
     "iopub.status.busy": "2025-02-23T13:42:03.548727Z",
     "iopub.status.idle": "2025-02-23T13:42:25.660975Z",
     "shell.execute_reply": "2025-02-23T13:42:25.659769Z",
     "shell.execute_reply.started": "2025-02-23T13:42:03.549353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install -U segmentation-models-pytorch\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:43:22.060013Z",
     "iopub.status.busy": "2025-02-23T13:43:22.059614Z",
     "iopub.status.idle": "2025-02-23T13:43:24.467077Z",
     "shell.execute_reply": "2025-02-23T13:43:24.466224Z",
     "shell.execute_reply.started": "2025-02-23T13:43:22.059985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class SDFModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SDFModel, self).__init__()\n",
    "        self.backbone = smp.DeepLabV3Plus(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=1,\n",
    "            classes=1\n",
    "        )\n",
    "        # self.backbone = smp.UnetPlusPlus(\n",
    "        #     encoder_name=\"efficientnet-b1\",   # or resnet34 for speed\n",
    "        #     encoder_weights=\"imagenet\",\n",
    "        #     in_channels=1,\n",
    "        #     classes=1,\n",
    "        # )\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)        # Output shape: (B, 1, H, W)\n",
    "        x = self.activation(x)      # Output in [-1, 1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:43:26.256756Z",
     "iopub.status.busy": "2025-02-23T13:43:26.256201Z",
     "iopub.status.idle": "2025-02-23T13:43:27.259602Z",
     "shell.execute_reply": "2025-02-23T13:43:27.258644Z",
     "shell.execute_reply.started": "2025-02-23T13:43:26.256726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )\n",
    "# model.add_module(\"activation\", nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SDFModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model_path = \"../../checkpoints/deeplabv3/model_20250620_211018/epoch_16\"\n",
    "print(\"Best model saved to: \", model_path)\n",
    "model.load_state_dict(torch.load(model_path))  \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = testDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_dataset)  # Check the length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    \"\"\" \n",
    "    Normalize input image to the range [-1, 1]\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    # Scale to [0, 1]\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-8)\n",
    "    \n",
    "    # Scale to [-1, 1]\n",
    "    image = image * 2 - 1\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Randomly select 10 images from the validation dataset\n",
    "indices = torch.randperm(len(validation_dataset))[:10]\n",
    "\n",
    "# Create a figure for plotting\n",
    "fig, axs = plt.subplots(10, 3, figsize=(15, 40))  # 10 rows, 3 columns\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get the ith sample\n",
    "        image, true_mask, image_path = validation_dataset[idx]\n",
    "\n",
    "        # Add batch dimension and transfer to the same device as model\n",
    "        image_batch = image.unsqueeze(0).to(device)\n",
    "\n",
    "        # Get model prediction\n",
    "        prediction = model(image_batch)\n",
    "        #predicted_mask = torch.argmax(prediction, dim=1).squeeze(0).cpu()\n",
    "\n",
    "        # Convert tensors to numpy arrays for plotting\n",
    "        image_np = image.squeeze().cpu().numpy()\n",
    "        true_mask_np = true_mask.squeeze().cpu().numpy()\n",
    "        predicted_mask_np = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "        # Plot original image\n",
    "        axs[i, 0].imshow(image_np, cmap='gray')\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 0].set_title(f'Original Image {idx}')\n",
    "\n",
    "        # # Plot true mask\n",
    "        # axs[i, 1].imshow(true_mask_np, cmap='gray')\n",
    "        # axs[i, 1].axis('off')\n",
    "        # axs[i, 1].set_title(f'True Mask {idx}')\n",
    "\n",
    "        # Plot predicted mask\n",
    "        axs[i, 1].imshow(predicted_mask_np, cmap='jet')\n",
    "        axs[i, 1].axis('off')\n",
    "        axs[i, 1].set_title(f'Predicted Mask {image_path.replace(\"Ovarian_Data_iq_process\", \"Ovarian_Data_iq_process/Masks\")}')\n",
    "\n",
    "\n",
    "        # Plot predicted mask\n",
    "        axs[i, 2].imshow(np.abs(normalize_image(predicted_mask_np))<.4, cmap='jet')\n",
    "        axs[i, 2].axis('off')\n",
    "        axs[i, 2].set_title(f'Predicted Mask contour')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(validation_dataset))):\n",
    "        image, mask, image_path = validation_dataset[i]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        prediction = model(image)\n",
    "        predicted_mask = prediction.squeeze().cpu().numpy()\n",
    "        predicted_mask = (np.abs(normalize_image(predicted_mask)) < .4).astype(np.uint8)\n",
    "\n",
    "        image = image.squeeze().cpu().numpy()\n",
    "\n",
    "        save_path_image = image_path.replace(\"washu_all_img\", \"washu_all_img/Images_SDF\")\n",
    "        save_path_mask = image_path.replace(\"washu_all_img\", \"washu_all_img/Masks_SDF3\")\n",
    "        os.makedirs(os.path.dirname(save_path_image), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(save_path_mask), exist_ok=True)\n",
    "        Image.fromarray((image * 255).astype(np.uint8)).save(save_path_image)  \n",
    "        Image.fromarray((predicted_mask * 255).astype(np.uint8)).save(save_path_mask)\n",
    "        print(f\"Saved image to {save_path_image} and mask to {save_path_mask}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 862050,
     "sourceId": 5144,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
